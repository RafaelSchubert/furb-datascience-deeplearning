{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee03272",
   "metadata": {},
   "source": [
    "# Classifying  IMDb Reviews with a RNN\n",
    "\n",
    "We'll be designing a RNN model that can classify a movie review from the [IMDb](https://www.imdb.com/) website as either positive (1) or negative (0).\n",
    "\n",
    "Start by importing the modules, types and functions that we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c890ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from math import ceil, log\n",
    "from nltk import WordNetLemmatizer\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torchtext import datasets\n",
    "from typing import Dict, Iterable, List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63714441",
   "metadata": {},
   "source": [
    "## 1) Load Dataset\n",
    "\n",
    "The dataset is available via the `torchtext` module (specifically, `torchtext.datasets` submodule). However, it's available as a sequence of tuples `(sentiment, review)`.\n",
    "\n",
    "We'll read the dataset from the source and transform it into a `pandas` `DataFrame` to better manipulate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e25157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_to_int(sentiment: str) -> int:\n",
    "    # 1 means 'positive', while 0 means 'negative'.\n",
    "    return int(sentiment.lower() == 'pos')\n",
    "\n",
    "def convert_dataset_tuple(dataset_tuple: Tuple[str, str]) -> Tuple[int, str]:\n",
    "    # Convert the sentiment tag into an integer.\n",
    "    # Make the review all lower case to normalize it.\n",
    "    (sentiment, review) = dataset_tuple\n",
    "    return (sentiment_to_int(sentiment), review.lower())\n",
    "\n",
    "def convert_dataset_to_list(dataset: Iterable[Tuple[str, str]]) -> List[Tuple[int, str]]:\n",
    "    # Convert all tuples to our specifications.\n",
    "    return list(map(convert_dataset_tuple, dataset))\n",
    "\n",
    "def convert_dataset_to_dataframe(dataset: Iterable[Tuple[str, str]]) -> pd.DataFrame:\n",
    "    # Build a pandas DataFrame from the converted tuples.\n",
    "    return pd.DataFrame(data=convert_dataset_to_list(dataset),\n",
    "                        columns=['sentiment', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0172b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_dataframes() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Read and transform both the train and test datasets.\n",
    "    (train_set_iter, test_set_iter) = datasets.IMDB()\n",
    "    return (convert_dataset_to_dataframe(train_set_iter),\n",
    "            convert_dataset_to_dataframe(test_set_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "652acd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_train, df_test) = load_imdb_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d19e6f",
   "metadata": {},
   "source": [
    "## 2) Analyse Dataset\n",
    "\n",
    "How many records are available in each dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4938b2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfd8bb2",
   "metadata": {},
   "source": [
    "Both the train and test datasets are the same size: 25,000 records.\n",
    "\n",
    "How do these records split between positive and negative reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0cf36da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5\n",
       "1    0.5\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e36da0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5\n",
       "1    0.5\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90adf177",
   "metadata": {},
   "source": [
    "Both datasets are balanced, 50% of each class. That's good, because it may relieve us from dealing with weights during the model training phase.\n",
    "\n",
    "But are there any empty reviews in those datasets? We may have to discard them, if that confirms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ccf35f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_str(value: str) -> str:\n",
    "    # Trim if not empty.\n",
    "    return value.strip() if value else value\n",
    "\n",
    "def is_str_null_or_empty(value: str) -> bool:\n",
    "    # Both bool('') and bool(None) evaluate to False.\n",
    "    return not trim_str(value)\n",
    "\n",
    "def is_str_series_complete(series: Iterable[str]) -> bool:\n",
    "    # If there's no single null/empty string,\n",
    "    # then the series is complete.\n",
    "    return not any(map(is_str_null_or_empty, series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb61e0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_str_series_complete(df_train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f431574d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_str_series_complete(df_test['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c0d097",
   "metadata": {},
   "source": [
    "No, there's no empty reviews in the datasets. The balance still remains.\n",
    "\n",
    "Normally, there'd be the need to analyse the datasets statistically, but they're all text-based. Other than the balance of the classes, there's nothing really left to look.\n",
    "\n",
    "Out of curiosity, what are the sizes of the shortest and longest reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99155455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_length_in_str_series(series: pd.Series) -> Tuple[int, int]:\n",
    "    # Call both pd.Series.min and pd.Series.max on the series.\n",
    "    minmax_length = series.apply(len).agg(['min', 'max'])\n",
    "    return (minmax_length['min'], minmax_length['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3398da8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 13704)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_length_in_str_series(df_train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0b903ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 12988)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_length_in_str_series(df_test['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a341490",
   "metadata": {},
   "source": [
    "There were definitely some very excited people reviewing those movies... (others, not so much)\n",
    "\n",
    "What do these reviews look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3ef6dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i rented i am curious-yellow from my video sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>\"i am curious: yellow\" is a risible and preten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>if only to avoid making this type of film in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>this film was probably inspired by godard's ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>oh, brother...after hearing about this ridicul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review\n",
       "0          0  i rented i am curious-yellow from my video sto...\n",
       "1          0  \"i am curious: yellow\" is a risible and preten...\n",
       "2          0  if only to avoid making this type of film in t...\n",
       "3          0  this film was probably inspired by godard's ma...\n",
       "4          0  oh, brother...after hearing about this ridicul..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92a623ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i love sci-fi and am willing to put up with a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>worth the entertainment value of a rental, esp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>its a totally average film with a few semi-alr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>star rating: ***** saturday night **** friday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>first off let me say, if you haven't enjoyed a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review\n",
       "0          0  i love sci-fi and am willing to put up with a ...\n",
       "1          0  worth the entertainment value of a rental, esp...\n",
       "2          0  its a totally average film with a few semi-alr...\n",
       "3          0  star rating: ***** saturday night **** friday ...\n",
       "4          0  first off let me say, if you haven't enjoyed a..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2480282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_first_n_reviews(dataframe: pd.DataFrame, n: int = 5) -> None:\n",
    "    for review in dataframe['review'][:n]:\n",
    "        print(f'\\t{review}\\n')\n",
    "\n",
    "def print_first_n_reviews_from_dataframes(n: int = 5) -> None:\n",
    "    for df in [df_train, df_test]:\n",
    "        print_first_n_reviews(df, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd43a9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ti rented i am curious-yellow from my video store because of all the controversy that surrounded it when it was first released in 1967. i also heard that at first it was seized by u.s. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" i really had to see this for myself.<br /><br />the plot is centered around a young swedish drama student named lena who wants to learn everything she can about life. in particular she wants to focus her attentions to making some sort of documentary on what the average swede thought about certain political issues such as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />what kills me about i am curious-yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman, arguably their answer to good old boy john ford, had sex scenes in his films.<br /><br />i do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in america. i am curious-yellow is a good film for anyone wanting to study the meat and potatoes (no pun intended) of swedish cinema. but really, this film doesn't have much of a plot.\n",
      "\n",
      "\t\"i am curious: yellow\" is a risible and pretentious steaming pile. it doesn't matter what one's political views are because this film can hardly be taken seriously on any level. as for the claim that frontal male nudity is an automatic nc-17, that isn't true. i've seen r-rated films with male nudity. granted, they only offer some fleeting views, but where are the r-rated films with gaping vulvas and flapping labia? nowhere, because they don't exist. the same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. and those pretentious indie movies like the brown bunny, in which we're treated to the site of vincent gallo's throbbing johnson, but not a trace of pink visible on chloe sevigny. before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. in fact, you generally won't see female genitals in an american film in anything short of porn or explicit erotica. this alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women's bodies.\n",
      "\n",
      "\tif only to avoid making this type of film in the future. this film is interesting as an experiment but tells no cogent story.<br /><br />one might feel virtuous for sitting thru it because it touches on so many important issues but it does so without any discernable motive. the viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />one might better spend one's time staring out a window at a tree growing.<br /><br />\n",
      "\n",
      "\tthis film was probably inspired by godard's masculin, féminin and i urge you to see that film instead.<br /><br />the film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. apart from that, what strikes me most is the endless stream of silliness. lena nyman has to be most annoying actress in the world. she acts so stupid and with all the nudity in this film,...it's unattractive. comparing to godard's film, intellectuality has been replaced with stupidity. without going too far on this subject, i would say that follows from the difference in ideals between the french and the swedish society.<br /><br />a movie of its time, and place. 2/10.\n",
      "\n",
      "\toh, brother...after hearing about this ridiculous film for umpteen years all i can think of is that old peggy lee song..<br /><br />\"is that all there is??\" ...i was just an early teen when this smoked fish hit the u.s. i was too young to get in the theater (although i did manage to sneak into \"goodbye columbus\"). then a screening at a local film museum beckoned - finally i could see this film, except now i was as old as my parents were when they schlepped to see it!!<br /><br />the only reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its u.s. release. millions of people flocked to this stinker, thinking they were going to see a sex film...instead, they got lots of closeups of gnarly, repulsive swedes, on-street interviews in bland shopping malls, asinie political pretension...and feeble who-cares simulated sex scenes with saggy, pale actors.<br /><br />cultural icon, holy grail, historic artifact..whatever this thing was, shred it, burn it, then stuff the ashes in a lead box!<br /><br />elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings..but if it weren't for the censorship scandal, it would have been ignored, then forgotten.<br /><br />instead, the \"i am blank, blank\" rhythymed title was repeated endlessly for years as a titilation for porno films (i am curious, lavender - for gay films, i am curious, black - for blaxploitation films, etc..) and every ten years or so the thing rises from the dead, to be viewed by a new generation of suckers who want to see that \"naughty sex film\" that \"revolutionized the film industry\"...<br /><br />yeesh, avoid like the plague..or if you must see it - rent the video and fast forward to the \"dirty\" parts, just to get it over with.<br /><br />\n",
      "\n",
      "\ti love sci-fi and am willing to put up with a lot. sci-fi movies/tv are usually underfunded, under-appreciated and misunderstood. i tried to like this, i really did, but it is to good tv sci-fi as babylon 5 is to star trek (the original). silly prosthetics, cheap cardboard sets, stilted dialogues, cg that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (i'm sure there are those of you out there who think babylon 5 is good sci-fi tv. it's not. it's clichéd and uninspiring.) while us viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. star trek). it may treat important issues, yet not as a serious philosophy. it's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. their actions and reactions are wooden and predictable, often painful to watch. the makers of earth know it's rubbish as they have to always say \"gene roddenberry's earth...\" otherwise people would not continue watching. roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging trabant of a show lumbers into space. spoiler. so, kill off a main character. and then bring him back as another actor. jeeez! dallas all over again.\n",
      "\n",
      "\tworth the entertainment value of a rental, especially if you like action movies. this one features the usual car chases, fights with the great van damme kick style, shooting battles with the 40 shell load shotgun, and even terrorist style bombs. all of this is entertaining and competently handled but there is nothing that really blows you away if you've seen your share before.<br /><br />the plot is made interesting by the inclusion of a rabbit, which is clever but hardly profound. many of the characters are heavily stereotyped -- the angry veterans, the terrified illegal aliens, the crooked cops, the indifferent feds, the bitchy tough lady station head, the crooked politician, the fat federale who looks like he was typecast as the mexican in a hollywood movie from the 1940s. all passably acted but again nothing special.<br /><br />i thought the main villains were pretty well done and fairly well acted. by the end of the movie you certainly knew who the good guys were and weren't. there was an emotional lift as the really bad ones got their just deserts. very simplistic, but then you weren't expecting hamlet, right? the only thing i found really annoying was the constant cuts to vds daughter during the last fight scene.<br /><br />not bad. not good. passable 4.\n",
      "\n",
      "\tits a totally average film with a few semi-alright action sequences that make the plot seem a little better and remind the viewer of the classic van dam films. parts of the plot don't make sense and seem to be added in to use up time. the end plot is that of a very basic type that doesn't leave the viewer guessing and any twists are obvious from the beginning. the end scene with the flask backs don't make sense as they are added in and seem to have little relevance to the history of van dam's character. not really worth watching again, bit disappointed in the end production, even though it is apparent it was shot on a low budget certain shots and sections in the film are of poor directed quality\n",
      "\n",
      "\tstar rating: ***** saturday night **** friday night *** friday morning ** sunday night * monday morning <br /><br />former new orleans homicide cop jack robideaux (jean claude van damme) is re-assigned to columbus, a small but violent town in mexico to help the police there with their efforts to stop a major heroin smuggling operation into their town. the culprits turn out to be ex-military, lead by former commander benjamin meyers (stephen lord, otherwise known as jase from east enders) who is using a special method he learned in afghanistan to fight off his opponents. but jack has a more personal reason for taking him down, that draws the two men into an explosive final showdown where only one will walk away alive.<br /><br />after until death, van damme appeared to be on a high, showing he could make the best straight to video films in the action market. while that was a far more drama oriented film, with the shepherd he has returned to the high-kicking, no brainer action that first made him famous and has sadly produced his worst film since derailed. it's nowhere near as bad as that film, but what i said still stands.<br /><br />a dull, predictable film, with very little in the way of any exciting action. what little there is mainly consists of some limp fight scenes, trying to look cool and trendy with some cheap slo-mo/sped up effects added to them that sadly instead make them look more desperate. being a mexican set film, director isaac florentine has tried to give the film a robert rodriguez/desperado sort of feel, but this only adds to the desperation.<br /><br />vd gives a particularly uninspired performance and given he's never been a robert de niro sort of actor, that can't be good. as the villain, lord shouldn't expect to leave the beeb anytime soon. he gets little dialogue at the beginning as he struggles to muster an american accent but gets mysteriously better towards the end. all the supporting cast are equally bland, and do nothing to raise the films spirits at all.<br /><br />this is one shepherd that's strayed right from the flock. *\n",
      "\n",
      "\tfirst off let me say, if you haven't enjoyed a van damme movie since bloodsport, you probably will not like this movie. most of these movies may not have the best plots or best actors but i enjoy these kinds of movies for what they are. this movie is much better than any of the movies the other action guys (segal and dolph) have thought about putting out the past few years. van damme is good in the movie, the movie is only worth watching to van damme fans. it is not as good as wake of death (which i highly recommend to anyone of likes van damme) or in hell but, in my opinion it's worth watching. it has the same type of feel to it as nowhere to run. good fun stuff!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_first_n_reviews_from_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a080e91",
   "metadata": {},
   "source": [
    "We can see that there are HTML line breaks amidst the reviews. We'll have to remove them.\n",
    "\n",
    "## 3) Process Datasets\n",
    "\n",
    "After playing around a little bit with the datasets, I came with the following strategy to transform the reviews into usable data for the classification model:\n",
    "\n",
    "1. Remove the HTML line breaks;\n",
    "2. Break the review into sentences;\n",
    "3. Break each sentence into tokens:\n",
    "4. Filter out any entirely non-alphanumerical token;\n",
    "5. Filter out stopwords;\n",
    "6. Lemmatize remaining words.\n",
    "\n",
    "As I played around with the datasets, I noticed that the line breaks are the only HTML tags in the reviews. That means we won't have to employ some complex module to clean any HTML, a simple regular expression search will do just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebb7a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_line_breaks(review: str) -> str:\n",
    "    # We split the review where there's a line break tag,\n",
    "    # and join all the pieces again, with a blank space in between.\n",
    "    html_line_break_pattern = r'<\\s*(?:/\\s*br|br\\s*/)\\s*>'\n",
    "    return ' '.join(re.split(html_line_break_pattern, review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67e4f1c",
   "metadata": {},
   "source": [
    "To break the reviews into sentences, and then into words, we'll use the `nltk` module, used for Natural Language processing. It provides both the `sent_tokenize()` and `word_tokenize()` functions to split strings into sentences and into words, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1aeb9f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sentences_from_review(review: str) -> str:\n",
    "    # Split into sentences, but keep only the non-empty ones.\n",
    "    yield from filter(bool, nltk.sent_tokenize(review))\n",
    "\n",
    "def gen_words_from_sentence(sentence: str) -> str:\n",
    "    # Split into words, but keep only the non-empty ones.\n",
    "    yield from filter(bool, nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef4a307",
   "metadata": {},
   "source": [
    "Here, we define the filtering of the non-alphanumerical only tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "612a2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_alphanumerical_token(token: str) -> bool:\n",
    "    # If it has any alphanumerical character,\n",
    "    # then it's an alphanumerical token.\n",
    "    return any(filter(str.isalnum, token))\n",
    "\n",
    "def filter_alphanumerical_tokens(token_iter: Iterable[str]) -> str:\n",
    "    # Keep only what is considered alphanumerical.\n",
    "    yield from filter(is_alphanumerical_token, token_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46202eb2",
   "metadata": {},
   "source": [
    "We'll use the stopwords set also from the `nltk` module to filter out the stopwords from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e8e91e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "# These ones aren't included in the set, so we'll add them.\n",
    "en_stopwords |= {\"n't\", \"'s\", \"'t\", \"'ve\"}\n",
    "\n",
    "def is_not_stopword(word: str) -> bool:\n",
    "    # Not in the set, not a stopword.\n",
    "    return word not in en_stopwords\n",
    "\n",
    "def filter_relevant_tokens(word_iter: Iterable[str]) -> str:\n",
    "    # Keep only non-stopwords.\n",
    "    yield from filter(is_not_stopword, word_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a720922c",
   "metadata": {},
   "source": [
    "We'll use the `WordNetLemmatizer`, also from the `nltk` module, to lemmatize the remaining tokens. It isn't quite powerful, but may do just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3becc6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "def gen_lemmatized_tokens(word_iter: Iterable[str]) -> str:\n",
    "    yield from map(lemmer.lemmatize, word_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd8064",
   "metadata": {},
   "source": [
    "Here we join everything together in the sequence we'll apply those transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15379465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentence(sentence: str) -> List[str]:\n",
    "    # Steps 3 through 6, as described previously.\n",
    "    tokens = gen_words_from_sentence(sentence)\n",
    "    alphanum_tokens = filter_alphanumerical_tokens(tokens)\n",
    "    relevant_tokens = filter_relevant_tokens(alphanum_tokens)\n",
    "    lemmas = gen_lemmatized_tokens(relevant_tokens)\n",
    "    return list(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "106d9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_normalized_sentences_from_review(review: str) -> List[str]:\n",
    "    # Step 2, then 3 through 6.\n",
    "    sentences = gen_sentences_from_review(review)\n",
    "    normalized_sentences = map(normalize_sentence, sentences)\n",
    "    yield from filter(bool, normalized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe503719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_review(review: str) -> List[List[str]]:\n",
    "    # Step 1, then 2 through 6\n",
    "    review = remove_html_line_breaks(review)\n",
    "    normalized_sentences = gen_normalized_sentences_from_review(review)\n",
    "    return list(normalized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaefdd11",
   "metadata": {},
   "source": [
    "Finally, it's just a matter of applying those transformations to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47a2cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_reviews(dataframe: pd.DataFrame, transformation) -> None:\n",
    "    dataframe['review'] = dataframe['review'].apply(transformation)\n",
    "\n",
    "def transform_reviews_in_dataframes(transformation) -> None:\n",
    "    for dataframe in [df_train, df_test]:\n",
    "        transform_reviews(dataframe, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dc183db",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_reviews_in_dataframes(normalize_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88cad0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[['rented', 'curious-yellow', 'video', 'store', 'controversy', 'surrounded', 'first', 'released', '1967.', 'also', 'heard', 'first', 'seized', 'u.s.', 'custom', 'ever', 'tried', 'enter', 'country', 'therefore', 'fan', 'film', 'considered', 'controversial', 'really', 'see'], ['plot', 'centered', 'around', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'want', 'learn', 'everything', 'life'], ['particular', 'want', 'focus', 'attention', 'making', 'sort', 'documentary', 'average', 'swede', 'thought', 'certain', 'political', 'issue', 'vietnam', 'war', 'race', 'issue', 'united', 'state'], ['asking', 'politician', 'ordinary', 'denizen', 'stockholm', 'opinion', 'politics', 'sex', 'drama', 'teacher', 'classmate', 'married', 'men'], ['kill', 'curious-yellow', '40', 'year', 'ago', 'considered', 'pornographic'], ['really', 'sex', 'nudity', 'scene', 'far', 'even', 'shot', 'like', 'cheaply', 'made', 'porno'], ['countryman', 'mind', 'find', 'shocking', 'reality', 'sex', 'nudity', 'major', 'staple', 'swedish', 'cinema'], ['even', 'ingmar', 'bergman', 'arguably', 'answer', 'good', 'old', 'boy', 'john', 'ford', 'sex', 'scene', 'film'], ['commend', 'filmmaker', 'fact', 'sex', 'shown', 'film', 'shown', 'artistic', 'purpose', 'rather', 'shock', 'people', 'make', 'money', 'shown', 'pornographic', 'theater', 'america'], ['curious-yellow', 'good', 'film', 'anyone', 'wanting', 'study', 'meat', 'potato', 'pun', 'intended', 'swedish', 'cinema'], ['really', 'film', 'much', 'plot']]\n",
      "\n",
      "\t[['curious', 'yellow', 'risible', 'pretentious', 'steaming', 'pile'], ['matter', 'one', 'political', 'view', 'film', 'hardly', 'taken', 'seriously', 'level'], ['claim', 'frontal', 'male', 'nudity', 'automatic', 'nc-17', 'true'], ['seen', 'r-rated', 'film', 'male', 'nudity'], ['granted', 'offer', 'fleeting', 'view', 'r-rated', 'film', 'gaping', 'vulva', 'flapping', 'labium'], ['nowhere', 'exist'], ['go', 'crappy', 'cable', 'show', 'schlongs', 'swinging', 'breeze', 'clitoris', 'sight'], ['pretentious', 'indie', 'movie', 'like', 'brown', 'bunny', \"'re\", 'treated', 'site', 'vincent', 'gallo', 'throbbing', 'johnson', 'trace', 'pink', 'visible', 'chloe', 'sevigny'], ['cry', 'implying', 'double-standard', 'matter', 'nudity', 'mentally', 'obtuse', 'take', 'account', 'one', 'unavoidably', 'obvious', 'anatomical', 'difference', 'men', 'woman', 'genitals', 'display', 'actress', 'appears', 'nude', 'said', 'man'], ['fact', 'generally', 'wo', 'see', 'female', 'genitals', 'american', 'film', 'anything', 'short', 'porn', 'explicit', 'erotica'], ['alleged', 'double-standard', 'le', 'double', 'standard', 'admittedly', 'depressing', 'ability', 'come', 'term', 'culturally', 'inside', 'woman', 'body']]\n",
      "\n",
      "\t[['avoid', 'making', 'type', 'film', 'future'], ['film', 'interesting', 'experiment', 'tell', 'cogent', 'story'], ['one', 'might', 'feel', 'virtuous', 'sitting', 'thru', 'touch', 'many', 'important', 'issue', 'without', 'discernable', 'motive'], ['viewer', 'come', 'away', 'new', 'perspective', 'unless', 'one', 'come', 'one', 'one', 'mind', 'wanders', 'invariably', 'pointless', 'film'], ['one', 'might', 'better', 'spend', 'one', 'time', 'staring', 'window', 'tree', 'growing']]\n",
      "\n",
      "\t[['film', 'probably', 'inspired', 'godard', 'masculin', 'féminin', 'urge', 'see', 'film', 'instead'], ['film', 'two', 'strong', 'element', '1', 'realistic', 'acting', '2', 'impressive', 'undeservedly', 'good', 'photo'], ['apart', 'strike', 'endless', 'stream', 'silliness'], ['lena', 'nyman', 'annoying', 'actress', 'world'], ['act', 'stupid', 'nudity', 'film', 'unattractive'], ['comparing', 'godard', 'film', 'intellectuality', 'replaced', 'stupidity'], ['without', 'going', 'far', 'subject', 'would', 'say', 'follows', 'difference', 'ideal', 'french', 'swedish', 'society'], ['movie', 'time', 'place'], ['2/10']]\n",
      "\n",
      "\t[['oh', 'brother', 'hearing', 'ridiculous', 'film', 'umpteen', 'year', 'think', 'old', 'peggy', 'lee', 'song'], ['early', 'teen', 'smoked', 'fish', 'hit', 'u.s.', 'young', 'get', 'theater', 'although', 'manage', 'sneak', 'goodbye', 'columbus'], ['screening', 'local', 'film', 'museum', 'beckoned', 'finally', 'could', 'see', 'film', 'except', 'old', 'parent', 'schlepped', 'see'], ['reason', 'film', 'condemned', 'anonymous', 'sand', 'time', 'obscenity', 'case', 'sparked', 'u.s.', 'release'], ['million', 'people', 'flocked', 'stinker', 'thinking', 'going', 'see', 'sex', 'film', 'instead', 'got', 'lot', 'closeup', 'gnarly', 'repulsive', 'swede', 'on-street', 'interview', 'bland', 'shopping', 'mall', 'asinie', 'political', 'pretension', 'feeble', 'who-cares', 'simulated', 'sex', 'scene', 'saggy', 'pale', 'actor'], ['cultural', 'icon', 'holy', 'grail', 'historic', 'artifact', 'whatever', 'thing', 'shred', 'burn', 'stuff', 'ash', 'lead', 'box'], ['elite', 'esthete', 'still', 'scrape', 'find', 'value', 'boring', 'pseudo', 'revolutionary', 'political', 'spewings', 'censorship', 'scandal', 'would', 'ignored', 'forgotten'], ['instead', 'blank', 'blank', 'rhythymed', 'title', 'repeated', 'endlessly', 'year', 'titilation', 'porno', 'film', 'curious', 'lavender', 'gay', 'film', 'curious', 'black', 'blaxploitation', 'film', 'etc', 'every', 'ten', 'year', 'thing', 'rise', 'dead', 'viewed', 'new', 'generation', 'sucker', 'want', 'see', 'naughty', 'sex', 'film', 'revolutionized', 'film', 'industry', 'yeesh', 'avoid', 'like', 'plague', 'must', 'see', 'rent', 'video', 'fast', 'forward', 'dirty', 'part', 'get']]\n",
      "\n",
      "\t[['love', 'sci-fi', 'willing', 'put', 'lot'], ['sci-fi', 'movies/tv', 'usually', 'underfunded', 'under-appreciated', 'misunderstood'], ['tried', 'like', 'really', 'good', 'tv', 'sci-fi', 'babylon', '5', 'star', 'trek', 'original'], ['silly', 'prosthetics', 'cheap', 'cardboard', 'set', 'stilted', 'dialogue', 'cg', 'match', 'background', 'painfully', 'one-dimensional', 'character', 'overcome', \"'sci-fi\", 'setting'], [\"'m\", 'sure', 'think', 'babylon', '5', 'good', 'sci-fi', 'tv'], ['clichéd', 'uninspiring'], ['u', 'viewer', 'might', 'like', 'emotion', 'character', 'development', 'sci-fi', 'genre', 'take', 'seriously', 'cf'], ['star', 'trek'], ['may', 'treat', 'important', 'issue', 'yet', 'serious', 'philosophy'], ['really', 'difficult', 'care', 'character', 'simply', 'foolish', 'missing', 'spark', 'life'], ['action', 'reaction', 'wooden', 'predictable', 'often', 'painful', 'watch'], ['maker', 'earth', 'know', 'rubbish', 'always', 'say', 'gene', 'roddenberry', 'earth', 'otherwise', 'people', 'would', 'continue', 'watching'], ['roddenberry', 'ash', 'must', 'turning', 'orbit', 'dull', 'cheap', 'poorly', 'edited', 'watching', 'without', 'advert', 'break', 'really', 'brings', 'home', 'trudging', 'trabant', 'show', 'lumber', 'space'], ['spoiler'], ['kill', 'main', 'character'], ['bring', 'back', 'another', 'actor'], ['jeeez'], ['dallas']]\n",
      "\n",
      "\t[['worth', 'entertainment', 'value', 'rental', 'especially', 'like', 'action', 'movie'], ['one', 'feature', 'usual', 'car', 'chase', 'fight', 'great', 'van', 'damme', 'kick', 'style', 'shooting', 'battle', '40', 'shell', 'load', 'shotgun', 'even', 'terrorist', 'style', 'bomb'], ['entertaining', 'competently', 'handled', 'nothing', 'really', 'blow', 'away', 'seen', 'share'], ['plot', 'made', 'interesting', 'inclusion', 'rabbit', 'clever', 'hardly', 'profound'], ['many', 'character', 'heavily', 'stereotyped', 'angry', 'veteran', 'terrified', 'illegal', 'alien', 'crooked', 'cop', 'indifferent', 'fed', 'bitchy', 'tough', 'lady', 'station', 'head', 'crooked', 'politician', 'fat', 'federale', 'look', 'like', 'typecast', 'mexican', 'hollywood', 'movie', '1940s'], ['passably', 'acted', 'nothing', 'special'], ['thought', 'main', 'villain', 'pretty', 'well', 'done', 'fairly', 'well', 'acted'], ['end', 'movie', 'certainly', 'knew', 'good', 'guy'], ['emotional', 'lift', 'really', 'bad', 'one', 'got', 'desert'], ['simplistic', 'expecting', 'hamlet', 'right'], ['thing', 'found', 'really', 'annoying', 'constant', 'cut', 'vd', 'daughter', 'last', 'fight', 'scene'], ['bad'], ['good'], ['passable', '4']]\n",
      "\n",
      "\t[['totally', 'average', 'film', 'semi-alright', 'action', 'sequence', 'make', 'plot', 'seem', 'little', 'better', 'remind', 'viewer', 'classic', 'van', 'dam', 'film'], ['part', 'plot', 'make', 'sense', 'seem', 'added', 'use', 'time'], ['end', 'plot', 'basic', 'type', 'leave', 'viewer', 'guessing', 'twist', 'obvious', 'beginning'], ['end', 'scene', 'flask', 'back', 'make', 'sense', 'added', 'seem', 'little', 'relevance', 'history', 'van', 'dam', 'character'], ['really', 'worth', 'watching', 'bit', 'disappointed', 'end', 'production', 'even', 'though', 'apparent', 'shot', 'low', 'budget', 'certain', 'shot', 'section', 'film', 'poor', 'directed', 'quality']]\n",
      "\n",
      "\t[['star', 'rating', 'saturday', 'night', 'friday', 'night', 'friday', 'morning', 'sunday', 'night', 'monday', 'morning', 'former', 'new', 'orleans', 'homicide', 'cop', 'jack', 'robideaux', 'jean', 'claude', 'van', 'damme', 're-assigned', 'columbus', 'small', 'violent', 'town', 'mexico', 'help', 'police', 'effort', 'stop', 'major', 'heroin', 'smuggling', 'operation', 'town'], ['culprit', 'turn', 'ex-military', 'lead', 'former', 'commander', 'benjamin', 'meyers', 'stephen', 'lord', 'otherwise', 'known', 'jase', 'east', 'enders', 'using', 'special', 'method', 'learned', 'afghanistan', 'fight', 'opponent'], ['jack', 'personal', 'reason', 'taking', 'draw', 'two', 'men', 'explosive', 'final', 'showdown', 'one', 'walk', 'away', 'alive'], ['death', 'van', 'damme', 'appeared', 'high', 'showing', 'could', 'make', 'best', 'straight', 'video', 'film', 'action', 'market'], ['far', 'drama', 'oriented', 'film', 'shepherd', 'returned', 'high-kicking', 'brainer', 'action', 'first', 'made', 'famous', 'sadly', 'produced', 'worst', 'film', 'since', 'derailed'], ['nowhere', 'near', 'bad', 'film', 'said', 'still', 'stand'], ['dull', 'predictable', 'film', 'little', 'way', 'exciting', 'action'], ['little', 'mainly', 'consists', 'limp', 'fight', 'scene', 'trying', 'look', 'cool', 'trendy', 'cheap', 'slo-mo/sped', 'effect', 'added', 'sadly', 'instead', 'make', 'look', 'desperate'], ['mexican', 'set', 'film', 'director', 'isaac', 'florentine', 'tried', 'give', 'film', 'robert', 'rodriguez/desperado', 'sort', 'feel', 'add', 'desperation'], ['vd', 'give', 'particularly', 'uninspired', 'performance', 'given', 'never', 'robert', 'de', 'niro', 'sort', 'actor', 'ca', 'good'], ['villain', 'lord', 'expect', 'leave', 'beeb', 'anytime', 'soon'], ['get', 'little', 'dialogue', 'beginning', 'struggle', 'muster', 'american', 'accent', 'get', 'mysteriously', 'better', 'towards', 'end'], ['supporting', 'cast', 'equally', 'bland', 'nothing', 'raise', 'film', 'spirit'], ['one', 'shepherd', 'strayed', 'right', 'flock']]\n",
      "\n",
      "\t[['first', 'let', 'say', 'enjoyed', 'van', 'damme', 'movie', 'since', 'bloodsport', 'probably', 'like', 'movie'], ['movie', 'may', 'best', 'plot', 'best', 'actor', 'enjoy', 'kind', 'movie'], ['movie', 'much', 'better', 'movie', 'action', 'guy', 'segal', 'dolph', 'thought', 'putting', 'past', 'year'], ['van', 'damme', 'good', 'movie', 'movie', 'worth', 'watching', 'van', 'damme', 'fan'], ['good', 'wake', 'death', 'highly', 'recommend', 'anyone', 'like', 'van', 'damme', 'hell', 'opinion', 'worth', 'watching'], ['type', 'feel', 'nowhere', 'run'], ['good', 'fun', 'stuff']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_first_n_reviews_from_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d136e7e",
   "metadata": {},
   "source": [
    "# 4) Modeling\n",
    "\n",
    "We now work on the definition of the classification model.\n",
    "\n",
    "## 4.1) Vocabulary\n",
    "\n",
    "We'll have to translate the words in the reviews into numbers so that our classification model is able understand the data we pass onto it. For that purpose, we'll build a _vocabulary_, or rather a class that reads a word and returns a corresponding integer. Later on, we'll translate these values to vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41a73cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, word_iter: Iterable[str]):\n",
    "        # Initialized with source from which to read all the words\n",
    "        # in the vocabulary.\n",
    "        self.index_table = Vocabulary.make_index_table_(word_iter)\n",
    "        self.padding_index = self.index_table[None]\n",
    "\n",
    "\n",
    "    def make_index_table_(word_iter: Iterable[str]) -> Dict[str, int]:\n",
    "        # Straightforward. If an word is not in the table, add it\n",
    "        # with a new corresponding integer value.\n",
    "        # The catch, however, is that we can't use 0, because we'll\n",
    "        # need it later on for padding.\n",
    "        table = { None: 0 }\n",
    "        for word in word_iter:\n",
    "            if word not in table:\n",
    "                table[word] = len(table)\n",
    "        return table\n",
    "\n",
    "\n",
    "    def translate_sentence(self, sentence: Iterable[str]) -> List[int]:\n",
    "        # Translate each sentence into a torch Tensor of integers.\n",
    "        return torch.tensor(list(map(self.translate_word_, sentence)), dtype=torch.long)\n",
    "\n",
    "\n",
    "    def translate_word_(self, word: str) -> int:\n",
    "        return self.index_table[word]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # Just so we can easily get the vocabulary's size.\n",
    "        return len(self.index_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdbe44e",
   "metadata": {},
   "source": [
    "Building the vocabulary that we'll use with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "873b97ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're considering all the words in both datasets.\n",
    "# That can become a problem, as some words may be present in one but not both.\n",
    "# But we'll carry on for now.\n",
    "imdb_vocab = Vocabulary(word\n",
    "                        for df in [df_train, df_test]\n",
    "                            for review in df['review']\n",
    "                                for sentence in review\n",
    "                                    for word in sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5931171e",
   "metadata": {},
   "source": [
    "## 4.2) Architecture\n",
    "\n",
    "For the architecture, I'm planning on using two LSTM (_Long Short-Term Memory_) layers to process two separate sequence in each review: the word sequence in each sentence, then the sentence sequence in each review.\n",
    "\n",
    "Truth be told, that's more of an experiment than anything else. I'm not entirely confident on the possible classification results. This approach may lead to a model that expects each sentence of a review to be influenced by the previous ones, which — although it could theoretically work — doesn't necessarily hold true.\n",
    "\n",
    "But there's no harm in trying it, is it? The worst it could happen is we ending up discovering another way of _\"how not to make a working light bulb.\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4ea594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    def __init__(self, vocab: Vocabulary, n_sentiments: int):\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "\n",
    "        vocab_len = len(vocab)\n",
    "\n",
    "        # \"Guess-timate\" the sizes of the layers.\n",
    "        # For the datasets we're working on, I believe they'll be approx.:\n",
    "        # - self.embedding_len = 6 or 7\n",
    "        # - self.hidden_len = 4\n",
    "        self.embedding_len = int(vocab_len ** (1 / (1 + log(vocab_len, 10))))\n",
    "        self.hidden_len = n_sentiments ** 2\n",
    "        self.n_sentiments = n_sentiments\n",
    "\n",
    "        # We're using Embedding to convert the words (integers) into numerical vectors.\n",
    "        self.word_embedder = nn.Embedding(vocab_len, self.embedding_len,\n",
    "                                          padding_idx=vocab.padding_index)\n",
    "\n",
    "        # We're using three layers:\n",
    "        # - a LSTM layer to process the word sequence of each sentence;\n",
    "        # - a LSTM layer to process the sentence sequence of each review;\n",
    "        # - a Linear layer to get the likelihood of each sentimente for a given review.\n",
    "        # The layers feed each other in that same sequence.\n",
    "        self.sentence_sentiment_layer = nn.LSTM(self.embedding_len, self.hidden_len, batch_first=True)\n",
    "        self.review_sentiment_layer = nn.LSTM(self.hidden_len, self.hidden_len, batch_first=True)\n",
    "        self.to_sentiment_layer = nn.Linear(self.hidden_len, self.n_sentiments)\n",
    "\n",
    "\n",
    "    def forward(self, reviews: Iterable[Iterable[torch.Tensor]]) -> torch.Tensor:\n",
    "        # We need to process the first layer and transform the result\n",
    "        # so that it can be read by the next layer.\n",
    "        reviews = self.reviews_to_review_layer_input_(reviews)\n",
    "\n",
    "        # Process the previous values.\n",
    "        X = self.process_review_layer_(reviews)\n",
    "\n",
    "        # Calculate the likelihood of each sentiment.\n",
    "        X = self.to_sentiment_layer(X)\n",
    "        X = torch.tanh(X)\n",
    "\n",
    "        # Get the negative log likelihood of each sentiment.\n",
    "        return nn.functional.log_softmax(X, dim=1)\n",
    "\n",
    "\n",
    "    def reviews_to_review_layer_input_(self, reviews: Iterable[Iterable[torch.Tensor]]) -> List[torch.Tensor]:\n",
    "        # Process each review and build a list with the results.\n",
    "        return list(map(self.process_sentences_layer_, reviews))\n",
    "\n",
    "\n",
    "    def process_sentences_layer_(self, sentences: Iterable[torch.Tensor]) -> torch.Tensor:\n",
    "        # The sentence length is variable, so we need to pad them to the longest length.\n",
    "        X = nn.utils.rnn.pad_sequence(sentences, batch_first=True)\n",
    "        # Then, we convert the integer-words into numerical vectors.\n",
    "        X = self.word_embedder(X)\n",
    "\n",
    "        # Pack the padded sentences to filter out the padded values.\n",
    "        # That's why we don't associate an word to 0 in the vocabulary.\n",
    "        sentences_lengths = list(map(len, sentences))\n",
    "        X = nn.utils.rnn.pack_padded_sequence(X, sentences_lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Initial values for the hidden and cell states.\n",
    "        hidden = torch.randn(1, len(sentences), self.hidden_len)\n",
    "        cell = torch.randn(1, len(sentences), self.hidden_len)\n",
    "        # Process the LSTM layer.\n",
    "        X, _ = self.sentence_sentiment_layer(X, (hidden, cell))\n",
    "\n",
    "        # Unpack the result, getting the padded values\n",
    "        # as well as their corresponding true lengths.\n",
    "        X, X_lengths = nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "\n",
    "        # To pick the last values in the processed word sequences,\n",
    "        # we partially flatten the result matrix and pick the corresponding indices.\n",
    "        flattened_indices = torch.arange(len(X_lengths)) * max(X_lengths) + (X_lengths - 1)\n",
    "        X = X.view(-1, self.hidden_len)[flattened_indices, :]\n",
    "\n",
    "        # Use TanH so that we get a better symmetry.\n",
    "        return torch.tanh(X)\n",
    "\n",
    "\n",
    "    def process_review_layer_(self, reviews: Iterable[torch.Tensor]) -> torch.Tensor:\n",
    "        # If you understood what was done in `process_sentences_layer_()`,\n",
    "        # then you may also understand this one, as they're very, very similar.\n",
    "\n",
    "        # Pad to length of the longest sentence sequence.\n",
    "        X = nn.utils.rnn.pad_sequence(reviews, batch_first=True)\n",
    "\n",
    "        # Pack the padded matrix.\n",
    "        reviews_lengths = list(map(len, reviews))\n",
    "        X = nn.utils.rnn.pack_padded_sequence(X, reviews_lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Initial values for the hidden and cell states.\n",
    "        hidden = torch.randn(1, len(reviews), self.hidden_len)\n",
    "        cell = torch.randn(1, len(reviews), self.hidden_len)\n",
    "        # Process the LSTM layer.\n",
    "        X, _ = self.review_sentiment_layer(X, (hidden, cell))\n",
    "\n",
    "        # Unpack the result.\n",
    "        X, X_lengths = nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "\n",
    "        # Partially flatten the result matrix and pick\n",
    "        # only the last values of each sequence.\n",
    "        flattened_indices = torch.arange(len(X_lengths)) * max(X_lengths) + (X_lengths - 1)\n",
    "        X = X.view(-1, self.hidden_len)[flattened_indices, :]\n",
    "\n",
    "        # TanH for better symmetry.\n",
    "        return torch.tanh(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d8df30",
   "metadata": {},
   "source": [
    "This function takes a list of reviews and transforms them so that they can be read by the model. We use the vocabulary we built previously to translate the words into intergers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fefbb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviews_to_model_input(reviews: Iterable[Iterable[Iterable[str]]]) -> List[List[torch.Tensor]]:\n",
    "    return [list(map(imdb_vocab.translate_sentence, r)) for r in reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97980bbf",
   "metadata": {},
   "source": [
    "# 5) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be84e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_classif = ReviewClassifier(imdb_vocab, len(['neg', 'pos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c599a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: ReviewClassifier, train_data: pd.DataFrame, *,\n",
    "                n_epochs: int = 1, batch_size: int = 64) -> None:\n",
    "    loss_criterion = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.03)\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        for batch in range(ceil(len(train_data) / batch_size)):\n",
    "            i_start = batch*batch_size\n",
    "            i_end = i_start+batch_size\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            X = reviews_to_model_input(train_data['review'][i_start:i_end])\n",
    "            Y = torch.from_numpy(train_data['sentiment'][i_start:i_end].values)\n",
    "\n",
    "            Y_pred = model(X)\n",
    "\n",
    "            batch_loss = loss_criterion(Y_pred, Y)\n",
    "            batch_loss.backward()\n",
    "\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e0f78ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(imdb_classif, df_train, n_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b98997a",
   "metadata": {},
   "source": [
    "# 6) Classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d98e1226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_model(model: ReviewClassifier, test_data: pd.DataFrame) -> None:\n",
    "    with torch.no_grad():\n",
    "        X = reviews_to_model_input(test_data['review'])\n",
    "        Y = torch.from_numpy(test_data['sentiment'].values)\n",
    "\n",
    "        Y_pred = model(X).argmax(dim=1)\n",
    "\n",
    "        return (Y_pred, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6762929",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Y_pred, Y_true) = predict_with_model(imdb_classif, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a3de51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "accuracy = (Y_pred == Y_true).sum() / len(Y_true)\n",
    "\n",
    "print(\"Accuracy:\", accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8180cd46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
